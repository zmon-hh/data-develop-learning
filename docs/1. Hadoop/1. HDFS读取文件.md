# HDFS读取文件流程
当我们创建并获取了一个DistributedFileSystem实例后，可以用它对HDFS中的文件或目录进行操作。

首先上一段代码，客户端是如何读文件的：
``` java
public static void main(String[] args) throws IOException {
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(conf);
    Path path = new Path("/hadoop-learning/output/part-r-00000");
    FSDataInputStream fsDataInputStream = fs.open(path);
    IOUtils.copyBytes(fsDataInputStream, System.out, 4096);
}
```
总体来说，最简单的HDFS读文件大体流程如下：
1. 获取文件系统实例FileSyStem，并通过其open()方法获取文件系统输入流inputStream；
   1. FileSyStem内部封装了一个dfs的客户端，在构建输入流inputStream时，输入流会通过dfs客户端连接namenode，获取HDFS文件的所有数据块的位置信息LocatedBlocks
2. 通过文件系统输入流inputStream读取数据（核心是通过blockReader读取数据）；
   1. inputStream对象会记录当前读取位置
   2. 通过位置计算当前正在读取的数据块，当到达下一个数据块会关闭上一个数据块的blockReader
   3. 选择当前数据块最优的datanode进行连接
   4. 创建一个blockReader

## 1. 如何获取一个文件的输入流FSDataInputStream

### 1.1 FileSystem.open()
``` java
public FSDataInputStream open(Path f, final int bufferSize) throws IOException {
    //文件系统读写过程中的一些统计，例如自从该HDFS对象建立以来，读了多少字节、写了多少字节等。
    this.statistics.incrementReadOps(1);
    this.storageStatistics.incrementOpCounter(OpType.OPEN);
    //相对路径转换为绝对路径
    Path absF = this.fixRelativePart(f);
    return (FSDataInputStream)(new FileSystemLinkResolver<FSDataInputStream>() {
        //核心方法
        public FSDataInputStream doCall(Path p) throws IOException, UnresolvedLinkException {
            //3个参数，getPathName表示要打开的文件路径，buffersize表示缓冲大小，verifyChecksum表示是否校验
            //此处的dfs为DFSCLient实例，在DistributedFileSystem的initialize方法中创建
            //通过DFSCLient的open方法构建DFSInputStream实例
            DFSInputStream dfsis = DistributedFileSystem.this.dfs.open(DistributedFileSystem.this.getPathName(p), bufferSize, DistributedFileSystem.this.verifyChecksum);
            //再通过DFSCLient的createWrappedInputStream方法将DFSInputStream封装成FSDataInputStream
            return DistributedFileSystem.this.dfs.createWrappedInputStream(dfsis);
        }

        public FSDataInputStream next(FileSystem fs, Path p) throws IOException {
            return fs.open(p, bufferSize);
        }
    }).resolve(this, absF);
}
```

### 1.2 进入dfs.open(String src, int buffersize, boolean verifyChecksum)方法

``` java
public DFSInputStream open(String src, int buffersize, boolean verifyChecksum) throws IOException, UnresolvedLinkException {
    //checkOpen()方法表示检查文件系统是否已经打开
    this.checkOpen();
    TraceScope scope = this.newPathTraceScope("newDFSInputStream", src);

    DFSInputStream var5;
    try {
        //发现，DFSInputStream输入流对象在此时构造
        var5 = new DFSInputStream(this, src, verifyChecksum);
    } finally {
        if (scope != null) {
            scope.close();
        }

    }

    return var5;
}
```
### 1.3 DFSInputStream构造方法
我们进入DFSInputStream构造方法，看看一个DFSInputStream对象是如何创建的。
``` java
DFSInputStream(DFSClient dfsClient, String src, boolean verifyChecksum) throws IOException, UnresolvedLinkException {
    //通过dfsClient实例对象，我们可以访问到NameNode实例，从而调用RPC从远程NameNode节点中获取需要的文件块信息和DataNode节点信息
    this.dfsClient = dfsClient;
    this.verifyChecksum = verifyChecksum;
    this.src = src;
    synchronized(this.infoLock) {
        this.cachingStrategy = dfsClient.getDefaultReadCachingStrategy();
    }
    //openInfo()方法是一个线程安全的方法，作用是从namenode获取要打开的文件的数据块信息。
    this.openInfo();
}
```
查看openInfo()方法
``` java
void openInfo() throws IOException, UnresolvedLinkException {
    synchronized(this.infoLock) {
        //主要方法
        this.lastBlockBeingWrittenLength = this.fetchLocatedBlocksAndGetLastBlockLength();

        int retriesForLastBlockLength;
        for(retriesForLastBlockLength = this.dfsClient.getConf().retryTimesForGetLastBlockLength; retriesForLastBlockLength > 0 && this.lastBlockBeingWrittenLength == -1L; --retriesForLastBlockLength) {
            DFSClient.LOG.warn("Last block locations not available. Datanodes might not have reported blocks completely. Will retry for " + retriesForLastBlockLength + " times");
            this.waitFor(this.dfsClient.getConf().retryIntervalForGetLastBlockLength);
            this.lastBlockBeingWrittenLength = this.fetchLocatedBlocksAndGetLastBlockLength();
        }

        if (this.lastBlockBeingWrittenLength == -1L && retriesForLastBlockLength == 0) {
            throw new IOException("Could not obtain the last block locations.");
        }
    }
}
```
该方法中如果读取数据块信息失败，则会再次读取3次，主要调用了方法fetchLocatedBlocksAndGetLastBlockLength()方法来读取数据块的信息。该方法读取数据块信息并且获得最后一个数据块的长度。为什么要获取最后一个数据块的长度呢？因为之前的数据块大小固定，如果是默认的，那就是128M，而最后一块大小就不一定了，需要获取。

### 1.4 进入fetchLocatedBlocksAndGetLastBlockLength()方法
``` java 
private long fetchLocatedBlocksAndGetLastBlockLength() throws IOException {
    //getLocatedBlocks()通过远程通信获取数据块信息
    LocatedBlocks newInfo = this.dfsClient.getLocatedBlocks(this.src, 0L);
    if (DFSClient.LOG.isDebugEnabled()) {
        DFSClient.LOG.debug("newInfo = " + newInfo);
    }

    if (newInfo == null) {
        throw new IOException("Cannot open filename " + this.src);
    } else {
        //locatedBlocks已经存在的处理方法
        if (this.locatedBlocks != null) {
            Iterator<LocatedBlock> oldIter = this.locatedBlocks.getLocatedBlocks().iterator();
            Iterator newIter = newInfo.getLocatedBlocks().iterator();

            while(oldIter.hasNext() && newIter.hasNext()) {
                if (!((LocatedBlock)oldIter.next()).getBlock().equals(((LocatedBlock)newIter.next()).getBlock())) {
                    throw new IOException("Blocklist for " + this.src + " has changed!");
                }
            }
        }
        //通过locatedBlocks记录数据块信息
        this.locatedBlocks = newInfo;
        //获得最后一个数据块的长度
        long lastBlockBeingWrittenLength = 0L;
        if (!this.locatedBlocks.isLastBlockComplete()) {
            LocatedBlock last = this.locatedBlocks.getLastLocatedBlock();
            if (last != null) {
                if (last.getLocations().length == 0) {
                    if (last.getBlockSize() == 0L) {
                        return 0L;
                    }

                    return -1L;
                }

                long len = this.readBlockLength(last);
                last.getBlock().setNumBytes(len);
                lastBlockBeingWrittenLength = len;
            }
        }

        this.fileEncryptionInfo = this.locatedBlocks.getFileEncryptionInfo();
        return lastBlockBeingWrittenLength;
    }
}
```

### 1.5 进入getLocatedBlocks(String src, long start, long length)方法
``` java 
public LocatedBlocks getLocatedBlocks(String src, long start) throws IOException {
    return this.getLocatedBlocks(src, start, this.dfsClientConf.prefetchSize);
}

@VisibleForTesting
public LocatedBlocks getLocatedBlocks(String src, long start, long length) throws IOException {
    TraceScope scope = this.newPathTraceScope("getBlockLocations", src);

    LocatedBlocks var7;
    try {
        //发现是该方法从namenode获取数据块信息
        var7 = callGetBlockLocations(this.namenode, src, start, length);
    } finally {
        if (scope != null) {
            scope.close();
        }

    }

    return var7;
}
```

### 1.6 进入callGetBlockLocations()方法
``` java
static LocatedBlocks callGetBlockLocations(ClientProtocol namenode, String src, long start, long length) throws IOException {
    try {
        //通过namenode获取BlockLocations
        return namenode.getBlockLocations(src, start, length);
    } catch (RemoteException var7) {
        throw var7.unwrapRemoteException(new Class[]{AccessControlException.class, FileNotFoundException.class, UnresolvedPathException.class});
    }
}
```
这里的namenode是NameNodeRpcServer实例


**初始化DFSInputStream的核心是通过dfs的namenode获取该HDFS文件的所有数据块的位置信息(LocatedBlocks)。**

LocatedBlocks的信息
``` 
LocatedBlocks{
  fileLength=545832960      文件总大小
  underConstruction=false
  blocks=[
      LocatedBlock{    第一个数据块信息
          BP-1326862926-192.168.59.101-1589199008553:
          blk_1073744207_3383; 
          getBlockSize()=134217728;  该文件块大小，默认128M
          corrupt=false; 
          offset=0; 
          locs=[   存放该数据块的DataNode节点
              DatanodeInfoWithStorage[192.168.59.102:50010,DS-43eb9e52-a20e-44c3-922c-c728f87514e9,DISK], 
              DatanodeInfoWithStorage[192.168.59.101:50010,DS-e0c1ada4-716c-4cfe-a233-dad576994443,DISK], 
              DatanodeInfoWithStorage[192.168.59.103:50010,DS-da366490-fe22-4fee-b030-d02e674dccf7,DISK]]}, 
      LocatedBlock{    第二个数据块信息
          BP-1326862926-192.168.59.101-1589199008553:blk_1073744208_3384; getBlockSize()=134217728; corrupt=false; offset=134217728; locs=[DatanodeInfoWithStorage[192.168.59.102:50010,DS-43eb9e52-a20e-44c3-922c-c728f87514e9,DISK], DatanodeInfoWithStorage[192.168.59.103:50010,DS-da366490-fe22-4fee-b030-d02e674dccf7,DISK], DatanodeInfoWithStorage[192.168.59.101:50010,DS-e0c1ada4-716c-4cfe-a233-dad576994443,DISK]]}, 
      LocatedBlock{    第三个数据块信息
          BP-1326862926-192.168.59.101-1589199008553:blk_1073744209_3385; getBlockSize()=134217728; corrupt=false; offset=268435456; locs=[DatanodeInfoWithStorage[192.168.59.103:50010,DS-da366490-fe22-4fee-b030-d02e674dccf7,DISK], DatanodeInfoWithStorage[192.168.59.101:50010,DS-e0c1ada4-716c-4cfe-a233-dad576994443,DISK], DatanodeInfoWithStorage[192.168.59.102:50010,DS-43eb9e52-a20e-44c3-922c-c728f87514e9,DISK]]}, 
      LocatedBlock{    第四个数据块信息
          BP-1326862926-192.168.59.101-1589199008553:blk_1073744210_3386; getBlockSize()=134217728; corrupt=false; offset=402653184; locs=[DatanodeInfoWithStorage[192.168.59.102:50010,DS-43eb9e52-a20e-44c3-922c-c728f87514e9,DISK], DatanodeInfoWithStorage[192.168.59.101:50010,DS-e0c1ada4-716c-4cfe-a233-dad576994443,DISK], DatanodeInfoWithStorage[192.168.59.103:50010,DS-da366490-fe22-4fee-b030-d02e674dccf7,DISK]]}, 
      LocatedBlock{    第五个数据块信息
          BP-1326862926-192.168.59.101-1589199008553:blk_1073744211_3387; getBlockSize()=8962048; corrupt=false; offset=536870912; locs=[DatanodeInfoWithStorage[192.168.59.102:50010,DS-43eb9e52-a20e-44c3-922c-c728f87514e9,DISK], DatanodeInfoWithStorage[192.168.59.101:50010,DS-e0c1ada4-716c-4cfe-a233-dad576994443,DISK], DatanodeInfoWithStorage[192.168.59.103:50010,DS-da366490-fe22-4fee-b030-d02e674dccf7,DISK]]}]
  lastLocatedBlock=    最后一个数据块信息，与第五个数据块相同
      LocatedBlock{
          BP-1326862926-192.168.59.101-1589199008553:blk_1073744211_3387; getBlockSize()=8962048; corrupt=false; offset=536870912; locs=[DatanodeInfoWithStorage[192.168.59.101:50010,DS-e0c1ada4-716c-4cfe-a233-dad576994443,DISK], DatanodeInfoWithStorage[192.168.59.103:50010,DS-da366490-fe22-4fee-b030-d02e674dccf7,DISK], DatanodeInfoWithStorage[192.168.59.102:50010,DS-43eb9e52-a20e-44c3-922c-c728f87514e9,DISK]]}
  isLastBlockComplete=true}
```


## 2.如何从输入流FSDataInputStream读取数据
要读取数据，即调用DFSInputStream的read()方法
### 2.1 进入read()方法
``` java
public synchronized int read(ByteBuffer buf) throws IOException {
    DFSInputStream.ReaderStrategy byteBufferReader = new DFSInputStream.ByteBufferStrategy(buf);
    int reqLen = buf.remaining();
    TraceScope scope = this.dfsClient.newReaderTraceScope("DFSInputStream#byteBufferRead", this.src, this.getPos(), reqLen);

    int var6;
    try {
        //发现，主要通过readWithStrategy方法读取数据
        //三个参数，字节缓冲Reader，offset，长度
        int retLen = this.readWithStrategy(byteBufferReader, 0, reqLen);
        if (retLen < reqLen) {
            this.dfsClient.addRetLenToReaderScope(scope, retLen);
        }

        var6 = retLen;
    } finally {
        scope.close();
    }

    return var6;
}
```

### 2.2 进入readWithStrategy(）方法
``` java
private synchronized int readWithStrategy(DFSInputStream.ReaderStrategy strategy, int off, int len) throws IOException {
    this.dfsClient.checkOpen();
    if (this.closed.get()) {
        throw new IOException("Stream closed");
    } else {
        //新建一个根据数据块获取datanode节点新的的map
        Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap = new HashMap();
        this.failures = 0;
        if (this.pos < this.getFileLength()) {
            int retries = 2;

            while(retries > 0) {
                try {
                    //根据当前数据块pos连接datanode节点
                    //1.当currentNode为空（即是要读取第一个文件块），我们则产生一个DataNode连接
                    //2.pos>blockEnd当我们读取完成一个文件块后pos会大于blockEnd,一般默认blockEnd为128M，即为134217727
                    if (this.pos > this.blockEnd || this.currentNode == null) {
                        this.currentNode = this.blockSeekTo(this.pos);
                    }

                    int realLen = (int)Math.min((long)len, this.blockEnd - this.pos + 1L);
                    synchronized(this.infoLock) {
                        if (this.locatedBlocks.isLastBlockComplete()) {
                            realLen = (int)Math.min((long)realLen, this.locatedBlocks.getFileLength() - this.pos);
                        }
                    }
                    //读数据方法
                    int result = this.readBuffer(strategy, off, realLen, corruptedBlockMap);
                    if (result < 0) {
                        throw new IOException("Unexpected EOS from the reader");
                    }

                    this.pos += (long)result;
                    if (this.dfsClient.stats != null) {
                        this.dfsClient.stats.incrementBytesRead((long)result);
                    }

                    int var8 = result;
                    return var8;
                } catch (ChecksumException var15) {
                    throw var15;
                } catch (IOException var16) {
                    this.checkInterrupted(var16);
                    if (retries == 1) {
                        DFSClient.LOG.warn("DFS Read", var16);
                    }

                    this.blockEnd = -1L;
                    if (this.currentNode != null) {
                        this.addToDeadNodes(this.currentNode);
                    }

                    --retries;
                    if (retries == 0) {
                        throw var16;
                    }
                } finally {
                    this.reportCheckSumFailure(corruptedBlockMap, this.currentLocatedBlock.getLocations().length);
                }
            }

            return -1;
        } else {
            return -1;
        }
    }
}
```
readWithStrategy()方法创建一个根据数据块获取datanode节点信息的map，如果数据失败，则会再次读取2次，然后根据pos连接datanode节点，最后通过readBuffer()读取数据

注意：pos默认值为0，blockEnd默认值为-1。  
pos每读一次数据，pos+=4096（缓存byte[]大小，处理最后一次读取字节可能不同）。  
blockEnd在获取DataNode块信息时直接设置为blockEnd+该数据块的大小（默认除了最后一块外，全为128M)，则值为128M-1

所以当一个数据块读取完毕后pos大小应该是pos+128M=0+128M，满足pos>blockEnd，进入if语句，进入blockSeekTo方法


### 2.3 blockSeekTo()方法
该方法是根据当前FSDataInputStream的pos，连接到所需的最佳的datanode。
``` java
private synchronized DatanodeInfo blockSeekTo(long target) throws IOException {
    if (target >= this.getFileLength()) {
        throw new IOException("Attempted to read past end of file");
    } else {
        //每当进入当前方法时，都是上一个文件块已经读取完毕。因为方法要产生一个新的文件块。所以生成一个新的文件块之前需要先关闭之前的文件块
        this.closeCurrentBlockReader();
        DatanodeInfo chosenNode = null;
        int refetchToken = 1;
        int refetchEncryptionKey = 1;
        boolean connectFailedOnce = false;

        while(true) {
            //计算所需要的数据块
            LocatedBlock targetBlock = this.getBlockAt(target, true);

            assert target == this.pos : "Wrong postion " + this.pos + " expect " + target;

            long offsetIntoBlock = target - targetBlock.getStartOffset();
            //这里实际调用DFSInputStream.DNAddrPair的getBestNodeDNAddrPair方法获取要连接的datanode节点
            DFSInputStream.DNAddrPair retval = this.chooseDataNode(targetBlock, (Collection)null);
            chosenNode = retval.info;
            InetSocketAddress targetAddr = retval.addr;
            StorageType storageType = retval.storageType;
            targetBlock = retval.block;

            try {
                ExtendedBlock blk = targetBlock.getBlock();
                Token<BlockTokenIdentifier> accessToken = targetBlock.getBlockToken();
                CachingStrategy curCachingStrategy;
                boolean shortCircuitForbidden;
                synchronized(this.infoLock) {
                    curCachingStrategy = this.cachingStrategy;
                    shortCircuitForbidden = this.shortCircuitForbidden();
                }
                //这里生成了blockReader对象，用于HDFS数据块读取
                this.blockReader = (new BlockReaderFactory(this.dfsClient.getConf())).setInetSocketAddress(targetAddr).setRemotePeerFactory(this.dfsClient).setDatanodeInfo(chosenNode).setStorageType(storageType).setFileName(this.src).setBlock(blk).setBlockToken(accessToken).setStartOffset(offsetIntoBlock).setVerifyChecksum(this.verifyChecksum).setClientName(this.dfsClient.clientName).setLength(blk.getNumBytes() - offsetIntoBlock).setCachingStrategy(curCachingStrategy).setAllowShortCircuitLocalReads(!shortCircuitForbidden).setClientCacheContext(this.dfsClient.getClientContext()).setUserGroupInformation(this.dfsClient.ugi).setConfiguration(this.dfsClient.getConfiguration()).setTracer(this.dfsClient.getTracer()).build();
                if (connectFailedOnce) {
                    DFSClient.LOG.info("Successfully connected to " + targetAddr + " for " + blk);
                }

                return chosenNode;
            } catch (IOException var20) {
                this.checkInterrupted(var20);
                if (var20 instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {
                    DFSClient.LOG.info("Will fetch a new encryption key and retry, encryption key was invalid when connecting to " + targetAddr + " : " + var20);
                    --refetchEncryptionKey;
                    this.dfsClient.clearDataEncryptionKey();
                } else if (refetchToken > 0 && tokenRefetchNeeded(var20, targetAddr)) {
                    --refetchToken;
                    this.fetchBlockAt(target);
                } else {
                    connectFailedOnce = true;
                    DFSClient.LOG.warn("Failed to connect to " + targetAddr + " for block " + targetBlock.getBlock() + ", add to deadNodes and continue. ", var20);
                    this.addToDeadNodes(chosenNode);
                }
            }
        }
    }
}
```

### 2.4 回到2.2进入readBuffer()方法
``` java
rivate synchronized int readBuffer(DFSInputStream.ReaderStrategy reader, int off, int len, Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap) throws IOException {
    boolean retryCurrentNode = true;

    while(true) {
        Object ioe;
        try {
            //在这里传入2.3生成的blockReader对象，返回读取结果
            return reader.doRead(this.blockReader, off, len);
        } catch (ChecksumException var8) {
            DFSClient.LOG.warn("Found Checksum error for " + this.getCurrentBlock() + " from " + this.currentNode + " at " + var8.getPos());
            ioe = var8;
            retryCurrentNode = false;
            this.addIntoCorruptedBlockMap(this.getCurrentBlock(), this.currentNode, corruptedBlockMap);
        } catch (IOException var9) {
            if (!retryCurrentNode) {
                DFSClient.LOG.warn("Exception while reading from " + this.getCurrentBlock() + " of " + this.src + " from " + this.currentNode, var9);
            }

            ioe = var9;
        }

        boolean sourceFound = false;
        if (retryCurrentNode) {
            sourceFound = this.seekToBlockSource(this.pos);
        } else {
            this.addToDeadNodes(this.currentNode);
            sourceFound = this.seekToNewSource(this.pos);
        }

        if (!sourceFound) {
            throw ioe;
        }

        retryCurrentNode = false;
    }
}
```

doRead()方法
``` java
public int doRead(BlockReader blockReader, int off, int len) throws ChecksumException, IOException {
    int nRead = blockReader.read(this.buf, off, len);
    DFSInputStream.this.updateReadStatistics(DFSInputStream.this.readStatistics, nRead, blockReader);
    return nRead;
}
```
通过blockReader直接读取数据到缓存中（this.buf，真正的数据读取）

### 2.5 通过输入流读取数据总结
DFSInputStream通过blockSeekTo方法根据当前读取位置，计算当前数据块编号，获取该数据块的需要连接的datanode节点信息，生成当前数据块的blockReader对象。

HDFS文件的读取都通过blockReader的doread方法进行。